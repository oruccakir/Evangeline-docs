---
title: "Stories15M Overview"
description: "Deep dive into the Stories15M Transformer LLM implementation for FPGA"
---

## Introduction

Stories15M is a 15-million parameter Transformer-based language model optimized for text generation. It implements the decoder-only architecture similar to GPT, with FPGA acceleration for key computational kernels.

## Architecture

The model uses a **Decoder-Only Transformer** architecture:

```
Token Input
    │
    ▼
┌─────────────────┐
│   Embedding     │  Token → 288-dim vector
└────────┬────────┘
         │
    ┌────▼────┐
    │ Layer 0 │──┐
    └────┬────┘  │
         ⋮      │ × 6 Layers
    ┌────▼────┐  │
    │ Layer 5 │──┘
    └────┬────┘
         │
┌────────▼────────┐
│  Final RMSNorm  │
└────────┬────────┘
         │
┌────────▼────────┐
│  Output Logits  │  → Vocabulary (32000 tokens)
└─────────────────┘
```

### Transformer Block

Each layer contains:

```
Input
  │
  ├───────────────────┐
  ▼                   │
RMSNorm               │
  │                   │
  ▼                   │
Multi-Head Attention  │  (6 heads, 48-dim each)
  │ ├─ RoPE           │
  │ └─ KV-Cache       │
  │                   │
  ▼                   │
Add ←─────────────────┘
  │
  ├───────────────────┐
  ▼                   │
RMSNorm               │
  │                   │
  ▼                   │
FFN (SwiGLU)          │  W1, W2, W3 projections
  │                   │
  ▼                   │
Add ←─────────────────┘
  │
  ▼
Output
```

---

## Model Parameters

| Parameter | Value |
|-----------|-------|
| Embedding Dimension | 288 |
| Number of Layers | 6 |
| Number of Heads | 6 |
| Head Dimension | 48 |
| FFN Hidden Dim | 768 |
| Vocabulary Size | 32,000 |
| Max Sequence Length | 256 |
| Total Parameters | ~15M |

---

## Directory Structure

```
stories15M/
├── CMakeLists.txt        # Build configuration
└── src/
    ├── main.cpp              # Entry point
    ├── decode.cpp            # Token-by-token decoding
    ├── context.hpp           # Inference state (activations)
    ├── weight.hpp            # Model weights structure
    ├── tensor.hpp            # CPU tensors
    ├── tensor_fpga.hpp       # FPGA tensor abstractions
    ├── device.cpp            # CPU/FPGA device abstraction
    ├── vocab.cpp             # Tokenizer
    ├── benchmark.cpp         # Performance benchmarking
    └── kernel_*.cpp          # HLS kernel sources
```

---

## FPGA Kernels

| Kernel | Function | Description |
|--------|----------|-------------|
| `kernel_matmul` | Matrix-Vector Multiply | Core attention & FFN computation |
| `kernel_rmsnorm` | RMS Normalization | Pre-attention and pre-FFN normalization |
| `kernel_rope` | Rotary Position Embedding | Position-aware Q/K transformation |
| `kernel_softmax` | Softmax | Attention score normalization |
| `kernel_add` | Residual Add | Skip connections |
| `kernel_mul` | Element-wise Multiply | SwiGLU activation |

---

## Key Features

### RoPE (Rotary Position Embeddings)

Instead of absolute position encodings, RoPE rotates query and key vectors based on their position, enabling better length generalization:

```cpp
q_out[i] = q[i] * cos[pos] - q[i+1] * sin[pos]
q_out[i+1] = q[i] * sin[pos] + q[i+1] * cos[pos]
```

### KV-Cache

For efficient autoregressive generation, key and value projections are cached:

```cpp
k_cache[layer][pos] = k_rotated
v_cache[layer][pos] = v_projected
```

This avoids recomputing K/V for all previous tokens at each step.

### SwiGLU Activation

The FFN uses SwiGLU instead of standard ReLU:

```cpp
ffn_out = (SiLU(W1 · x) ⊙ W3 · x) · W2
```

---

## Building

Before running inference, you need to build the application. See the [Build & Configuration](/architecture/build-and-config) guide for full details. For cpu build below instructions are enough.

### CPU-Only Build (Quick Test)

```bash
cd stories15M
mkdir -p build && cd build
cmake ..
make -j$(nproc)
```

### FPGA Build

```bash
./build.sh --config=build_configs/tinyllama_hw_build_v0/.yaml
```

<Tip>
The FPGA build takes 1 hour. Use CPU builds for development and testing.
</Tip>

---

## Running Inference

### CPU Mode

For quick testing without FPGA hardware:

```bash
./main \
    --weight_path ../../models/stories15M/weights.bin \
    --vocab_path  ../../models/stories15M/tokenizer.bin \
    --max_seq 256 \
    --temp 0.8
```

The model will generate a story token by token.

### FPGA Mode

For hardware-accelerated inference on ZCU102:

```bash
# On the board (after deployment)
./stories15M_host \
    --weight_path /run/media/mmcblk0p1/stories15m/weights.bin \
    --vocab_path /run/media/mmcblk0p1/stories15m/tokenizer.bin \
    --xclbin /run/media/mmcblk0p1/stories15m/binary_container_1.xclbin \
    --max_seq 256 \
    --temp 0.8
```

<Note>
The `--xclbin` parameter triggers FPGA mode. If omitted, the application runs on CPU.
</Note>

---

## Configuration Parameters

| Parameter | Description | Default |
|-----------|-------------|---------|
| `--weight_path` | Path to model weights binary | Required |
| `--vocab_path` | Path to tokenizer/vocabulary file | Required |
| `--xclbin` | Path to FPGA binary (`.xclbin`) | CPU mode if omitted |
| `--max_seq` | Maximum sequence length for generation | 256 |
| `--temp` | Sampling temperature (higher = more random) | 0.8 |
| `--color` | Enable colored terminal output | Off |
| `--print_softmax` | Print token probabilities | Off |

### Temperature Guide

| Value | Behavior |
|-------|----------|
| 0.0 | Greedy (always pick the most likely token) |
| 0.5 | Balanced creativity |
| 0.8 | Default, good for stories |
| 1.0+ | Very creative/random |
